{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Splits\n",
    "## indices from data:\n",
    "1, 2, 4, 5, 9, 12\n",
    "## in the featlists: \n",
    "0 = time, 1= Heart rate, 2 = Breathing rate, 3 = posture, 4 = Activity, 5 = HRconf\n",
    "\n",
    "0 = Time\n",
    "1 = HR \n",
    "2 =BR\n",
    "3=SkinTemp\n",
    "4=Posture\n",
    "5=Activity\n",
    "6=PeakAccel\n",
    "7=BRAmplitude\n",
    "8=BRNoise\n",
    "9=BRConfidence\n",
    "10=ECGAmplitude\n",
    "11=ECGNoise\n",
    "12=HRConfidence\n",
    "13=HRV\n",
    "14=GSR\n",
    "15=ROGState\n",
    "16=ROGTime\n",
    "17=VerticalMin\n",
    "18=VerticalPeak\n",
    "19=LateralMin\n",
    "20=LateralPeak\n",
    "21=SagittalMin\n",
    "22=SagittalPeak\n",
    "23=AuxADC1\n",
    "24=AuxADC2\n",
    "25=AuxADC3\n",
    "\n",
    "req_full = 0,1,2,4,5,6,10,11,12,13,15,16,17,18,19,20,21,22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37.799999999999997, 7.5) -- (29.199999999999999, 6.2999999999999998) -- (22.199999999999999, 10.6) -- (30.199999999999999, 11.800000000000001) -- (35.600000000000001, 10.6) -- (28.399999999999999, 20.600000000000001)\n",
      "(1.03, 0.01) -- (0.57999999999999996, 0.01) -- (0.46999999999999997, 0.01) -- (0.45000000000000001, 0.01) -- (0.67000000000000004, 0.0) -- (0.44, 0.029999999999999999)\n",
      "(84.0, -25.0) -- (60.0, -31.0) -- (20.0, -75.0) -- (-3.0, -33.0) -- (85.0, -38.0) -- (7.0, -15.0)\n",
      "[[  1.35875928e+12   1.01000000e+02   1.86000000e+01 ...,   7.20000000e-01\n",
      "   -5.30000000e-01   3.10000000e-01]\n",
      " [  1.35875928e+12   1.01000000e+02   1.70000000e+01 ...,   7.70000000e-01\n",
      "   -7.10000000e-01  -1.00000000e-01]\n",
      " [  1.35875928e+12   1.01000000e+02   1.70000000e+01 ...,   2.40000000e-01\n",
      "   -3.10000000e-01  -1.00000000e-02]\n",
      " ..., \n",
      " [  1.35877812e+12   1.39000000e+02   2.58000000e+01 ...,   1.80000000e-01\n",
      "   -6.00000000e-02   8.60000000e-01]\n",
      " [  1.35877812e+12   1.38000000e+02   2.58000000e+01 ...,   2.10000000e-01\n",
      "   -9.10000000e-01   2.20000000e-01]\n",
      " [  1.35877812e+12   1.37000000e+02   2.60000000e+01 ...,   2.10000000e-01\n",
      "   -9.10000000e-01  -6.90000000e-01]]\n"
     ]
    }
   ],
   "source": [
    "def load_feat(filename):\n",
    "    Data = np.genfromtxt(filename,dtype=float, delimiter=',', skip_header=1)\n",
    "    return Data\n",
    "def load_label(filename):\n",
    "    Data = np.genfromtxt(filename,dtype=\"U20\", delimiter=',', skip_header=1)\n",
    "    return Data\n",
    "trainfeat = load_feat('train_feat.csv')\n",
    "trainlabel = load_label('train_label.csv')\n",
    "\n",
    "\n",
    "req_full = np.array([[i[0],i[1],i[2],i[4], i[5],i[6],i[10],i[11],i[12],i[15],i[16],i[17],i[18],i[19],i[20],i[21],i[22]] for i in trainfeat])\n",
    "req = [[i[0],i[1],i[2],i[4], i[5], i[12]] for i in trainfeat]\n",
    "snowlabel = np.array([i[0] for i in trainlabel if i[1] == \"snowboarding\"])\n",
    "sitlabel = np.array([i[0] for i in trainlabel if i[1] == \"sitting\"])\n",
    "lyinglabel = np.array([i[0] for i in trainlabel if i[1] == \"lying\"])\n",
    "liftlabel = np.array([i[0] for i in trainlabel if i[1] == \"lift\"])\n",
    "standlabel = np.array([i[0] for i in trainlabel if i[1] == \"standing\"])\n",
    "towlabel = np.array([i[0] for i in trainlabel if i[1] == \"towlift\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "snowfeat = np.array([i for i in req if i[0] in snowlabel.astype(float)])\n",
    "sitfeat = np.array([ i for i in req if i[0] in sitlabel.astype(float) ])\n",
    "lyingfeat = np.array([i for i in req if i[0] in lyinglabel.astype(float)])\n",
    "liftfeat = np.array([ i for i in req if i[0] in liftlabel.astype(float) ])\n",
    "standfeat = np.array([i for i in req if i[0] in standlabel.astype(float)])\n",
    "towfeat = np.array([i for i in req if i[0] in towlabel.astype(float)])\n",
    "\n",
    "brsnow = max(snowfeat[:,2]), min(snowfeat[:,2])\n",
    "brsit = max(np.array(sitfeat)[:,2].astype(float)), min(np.array(sitfeat)[:,2].astype(float))\n",
    "brlying = max(np.array(lyingfeat)[:,2].astype(float)), min(np.array(lyingfeat)[:,2].astype(float))\n",
    "brlift = max(np.array(liftfeat)[:,2].astype(float)), min(np.array(liftfeat)[:,2].astype(float))\n",
    "brstand= max(np.array(standfeat)[:,2].astype(float)), min(np.array(standfeat)[:,2].astype(float))\n",
    "brtow= max(np.array(towfeat)[:,2].astype(float)), min(np.array(towfeat)[:,2].astype(float))\n",
    "\n",
    "print(brsnow, \"--\",\n",
    "     brsit, \"--\",\n",
    "     brlying, \"--\",\n",
    "     brlift, \"--\",\n",
    "     brstand, \"--\",\n",
    "     brtow,)\n",
    "\n",
    "actsnow = max(np.array(snowfeat)[:,4].astype(float)), min(np.array(snowfeat)[:,4].astype(float))\n",
    "actsit = max(np.array(sitfeat)[:,4].astype(float)), min(np.array(sitfeat)[:,4].astype(float))\n",
    "actlying =  max(np.array(lyingfeat)[:,4].astype(float)), min(np.array(lyingfeat)[:,4].astype(float))\n",
    "actlift =  max(np.array(liftfeat)[:,4].astype(float)), min(np.array(liftfeat)[:,4].astype(float))\n",
    "actstand = max(np.array(standfeat)[:,4].astype(float)), min(np.array(standfeat)[:,4].astype(float))\n",
    "acttow = max(np.array(towfeat)[:,4].astype(float)), min(np.array(towfeat)[:,4].astype(float))\n",
    "\n",
    "print(actsnow, \"--\",\n",
    "     actsit, \"--\",\n",
    "     actlying, \"--\",\n",
    "     actlift, \"--\",\n",
    "     actstand, \"--\",\n",
    "     acttow)\n",
    "\n",
    "\n",
    "possnow = max(np.array(snowfeat)[:,3].astype(float)), min(np.array(snowfeat)[:,3].astype(float))\n",
    "possit= max(np.array(sitfeat)[:,3].astype(float)), min(np.array(sitfeat)[:,3].astype(float))\n",
    "poslying = max(np.array(lyingfeat)[:,3].astype(float)), min(np.array(lyingfeat)[:,3].astype(float))\n",
    "poslift = max(np.array(liftfeat)[:,3].astype(float)), min(np.array(liftfeat)[:,3].astype(float))\n",
    "posstand = max(np.array(standfeat)[:,3].astype(float)), min(np.array(standfeat)[:,3].astype(float))\n",
    "postow = max(np.array(towfeat)[:,3].astype(float)), min(np.array(towfeat)[:,3].astype(float))\n",
    "\n",
    "print(possnow, \"--\",\n",
    "     possit, \"--\",\n",
    "     poslying, \"--\",\n",
    "     poslift, \"--\",\n",
    "     posstand, \"--\",\n",
    "     postow )\n",
    "# before\n",
    "# (37.8, 7.5) -- (29.2, 6.3) -- (23.7, 9.6) -- (30.2, 11.8) -- (35.6, 10.6)\n",
    "# (0.84, 0.01) -- (0.58, 0.01) -- (0.47, 0.01) -- (0.45, 0.01) -- (0.67, 0.0)\n",
    "# (84.0, -22.0) -- (60.0, -31.0) -- (20.0, -75.0) -- (-3.0, -33.0) -- (83.0, -38.0)\n",
    "\n",
    "print(req_full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16021, 17)\n",
      "[  0.   1.   2.   4.   5.   6.  10.  11.  12.  15.  16.  17.  18.  19.  20.\n",
      "  21.  22.]\n",
      "(16021, 10)\n",
      "[  0.   1.   2.   4.  12.  15.  16.  17.  21.  22.]\n",
      "(16021, 7)\n",
      "[  0.   1.   2.   4.  12.  15.  16.]\n",
      "(16021, 7)\n",
      "[  0.   1.   2.   4.  12.  15.  16.]\n",
      "(16021, 7)\n",
      "[  0.   1.   2.   4.  12.  15.  16.]\n",
      "(16021, 7)\n",
      "[  0.   1.   2.   4.  12.  15.  16.]\n",
      "(16021, 7)\n",
      "[  0.   1.   2.   4.  12.  15.  16.]\n",
      "(16021, 6)\n",
      "[  0.   1.   2.   4.  12.  16.]\n",
      "(16021, 6)\n",
      "[  0.   1.   2.   4.  12.  16.]\n",
      "(16021, 6)\n",
      "[  0.   1.   2.   4.  12.  16.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "indexlist = [0,1,2,4,5,6,10,11,12,15,16,17,18,19,20,21,22]\n",
    "             \n",
    "req_index = np.vstack([req_full,indexlist])\n",
    "# np.append([[req_full]],[0,1,2,4,5,6,7,10,11,12,13,15,16,17,18,19,20,21,22])\n",
    "# np.array([req_full,np.array([0,1,2,4,5,6,7,10,11,12,13,15,16,17,18,19,20,21,22])])\n",
    "# print(req_index)\n",
    "# print(req_index)\n",
    "\n",
    "for thres in range(10):\n",
    "    sel = VarianceThreshold(threshold=(thres/10))\n",
    "    Y = sel.fit_transform(req_index)\n",
    "    print(Y.shape)\n",
    "    print(Y[-1])\n",
    "    \n",
    "   \n",
    "# [\"0 = Time\",\n",
    "# \"1 = HR\", \n",
    "# \"2 =BR\",\n",
    "# \"3=SkinTemp\",\n",
    "# \"4=Posture\",\n",
    "# \"5=Activity\",\n",
    "# \"6=PeakAccel\",\n",
    "# \"7=BRAmplitude\",\n",
    "# \"8=BRNoise\",\n",
    "# \"9=BRConfidence\",\n",
    "# \"10=ECGAmplitude\",\n",
    "# \"11=ECGNoise\",\n",
    "# \"12=HRConfidence\",\n",
    "# \"13=HRV\",\n",
    "# \"14=GSR\",\n",
    "# \"15=ROGState\",\n",
    "# \"16=ROGTime\",\n",
    "# \"17=VerticalMin\",\n",
    "# \"18=VerticalPeak\",\n",
    "# \"19=LateralMin\",\n",
    "# \"20=LateralPeak\",\n",
    "# \"21=SagittalMin\",\n",
    "# \"22=SagittalPeak\",\n",
    "# \"23=AuxADC1\",\n",
    "# \"24=AuxADC2\",\n",
    "# \"25=AuxADC3\"]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def seconds_to_minutes(data,filename):\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "        \n",
    "    # open the (csv) file\n",
    "    f = open(filename, 'w+')\n",
    "\n",
    "\n",
    "    for i in range(0,16020,60):\n",
    "\n",
    "        part = data[i:i+60]\n",
    "        mean = part.mean().astype(str)\n",
    "        meantime = str(data[i][0])[:-2]\n",
    "        # reset the line\n",
    "        line = \"\"\n",
    "        \n",
    "        # create the line of values\n",
    "        for value in mean:\n",
    "            line += str(value) + \",\"\n",
    "        line = line[:-1] + \"\\n\"\n",
    "\n",
    "        # write the line to the file\n",
    "        f.write(line)\n",
    "        \n",
    "    # close the file\n",
    "    f.close()\n",
    "\n",
    "seconds_to_minutes(trainfeat, \"minutes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the cost function [2 pt]\n",
    "\n",
    "The cost function is defined as the sum of the squared errors of each prediction\n",
    "\n",
    "(2.16) $$E(w_1, w_0|X) = \\frac{1}{N}\\sum^N_{t=1} [r^t - (w_1x^t + w_0)]^2$$\n",
    "\n",
    "*These videos are great for building intuition on the relation between the hypothesis function and the associated cost of that hypothesis for the data.*\n",
    "* [Cost function 1](https://www.youtube.com/watch?v=EANr4YttXIQ&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=6)\n",
    "* [Cost function 2](https://www.youtube.com/watch?v=J5vJFwQWOaY&list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&index=7)\n",
    "\n",
    "Write a function to compute the cost based on the dataset $X$, $R$ and parameters $w_0$ and $w_1$. Based on your plot of the data, try to estimate some sensible values for $w_0$ and $w_1$ and compute the corresponding cost. Try at least 3 different guesses and print their cost. Order the prints of your guesses from highest to lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25524.1209718\n",
      "28121.9327515\n",
      "38646.1813128\n"
     ]
    }
   ],
   "source": [
    "X = [i[0] for i in load_file('points.csv')]\n",
    "R = [i[1] for i in load_file('points.csv')]\n",
    "\n",
    "def linear_cost(w0, w1, X, R):\n",
    "    cost_total = 0\n",
    "    for i in range(len(X)):\n",
    "        cost_total += (R[i] - linear_model(w0, w1, X[i]))**2\n",
    "        \n",
    "    return (1/len(X)) * cost_total    \n",
    "\n",
    "print(linear_cost(50, 60, X, R))\n",
    "print(linear_cost(30, 40, X, R))\n",
    "print(linear_cost(10, 20, X, R))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the linear model [4 pt]\n",
    "\n",
    "We can find the minimum value of the cost function by taking the partial derivatives of that cost function for both of the weights $w_0$ and $w_1$ and setting them equal to $0$, resulting in the equations\n",
    "\n",
    "(2.17a) $$w_1 = \\frac{\\sum_tx^tr^t - \\bar{x}\\bar{r}N}{\\sum_t(x^t)^2 - N\\bar{x}^2}$$\n",
    "(2.17b) $$w_0 = \\bar{r} - w_1\\bar{x}$$\n",
    "\n",
    "You can compute the partial derivates of equation *2.16* yourself and set them both equal to zero, to check you understand where these two equations come from. Minimizing the cost function gives us the best possible parameters for a linear model predicting the values of the provided dataset. *Note:* If you are unfamiliar with the notation $\\bar{x}$, it is defined in *Alpaydin* too, below equation *2.17*.\n",
    "\n",
    "Write a function which computes the optimal values of $w_0$ and $w_1$ for a dataset consisting of the vectors $X$ and $R$, containing $N$ elements each. Use *matplotlib* again to plot the points, but now also add the line representing the hypothesis function you found. As the line is linear, you can simply plot it by computing the 2 end points and have *matplotlib* draw the connecting line.\n",
    "\n",
    "Note that with some clever [array operations](https://docs.scipy.org/doc/numpy/reference/routines.array-manipulation.html) and [linear algebra](https://docs.scipy.org/doc/numpy/reference/routines.linalg.html) you can avoid explicitly looping over all the elements in $X$ and $R$ in `linear_fit`, which will make you code a lot faster. However, this is just an optional extra and any working implementation of the equations above will be considered correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c1950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear_fit(X, R, N):\n",
    "    w1 = (np.sum(np.matmul(X,R)-(np.mean(X)*np.mean(R)*N))/(np.sum(X**2 - (N * (np.mean(X)**2)))))\n",
    "    w0 = (np.mean(R) - w1*np.mean(X))\n",
    "    plt.plot([0,w0],[0,w1])\n",
    "    \n",
    "linear_fit(np.array(X), np.array(R), len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Polynomial data [3 pt]\n",
    "\n",
    "The linear model can easily be extended to polynomials of any order by expanding the original input with the squared input $x^2$, the cubed input $x^3$, etc and adding additional weights to the model. For ease of calculation, the input is also expanded with a vector of $1$'s, to represent the input for the constant parameter $w_0$. The parameters then become $w_0$, $w_1$, $w_2$, etc., one factor for each term of the polynomial.\n",
    "\n",
    "So if originally the dataset of $N$ elements is of the form $X$ (superscripts are indices here)\n",
    "\n",
    "$$ X = \\left[\\begin{array}{c} x^1 \\\\ x^2 \\\\ \\vdots \\\\ x^N \\end{array} \\right]$$\n",
    "\n",
    "Then the matrix $D$ for a $k^{th}$-order polynomial becomes\n",
    "\n",
    "$$ D = \\left[\\begin{array}{cccc}\n",
    "1 & x^1 & (x^1)^2 & \\cdots & (x^1)^k \\\\ \n",
    "1 & x^2 & (x^2)^2 & \\cdots & (x^2)^k \\\\ \n",
    "\\vdots \\\\\n",
    "1 & x^N & (x^N)^2 & \\cdots & (x^N)^k \\\\ \n",
    "\\end{array} \\right]$$\n",
    "\n",
    "Write a function `create_D_matrix` that constructs this matrix for a given vector $X$ up the specified order $k$. Looking at plots for the dataset we have been using so far, the relationship between the points will probably be at least be quadratic. Use the function to construct a matrix $D$ of order $2$, print the matrix and verify that it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3]\n",
      " [1 2]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "def create_D_matrix(X, k):\n",
    "    D_mat = [[X[i]**j for j in range(k)] for i in range(len(X)) ]  \n",
    "    return np.array(D_mat)\n",
    "D = create_D_matrix([3,2,3],2)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial model [2 pt]\n",
    "\n",
    "The parameters can now be represented as\n",
    "\n",
    "$$ w = \\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_k \\end{array} \\right]$$\n",
    "\n",
    "The hypothesis for a single input then just becomes\n",
    "\n",
    "$$ g(x^1) = \\sum_{i=0}^k D^1_iw_i $$\n",
    "\n",
    "Which can write as a matrix multiplication for all inputs in a single equation\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "1 & x^1 & (x^1)^2 & \\cdots & (x^1)^k \\\\ \n",
    "1 & x^2 & (x^2)^2 & \\cdots & (x^2)^k \\\\ \n",
    "\\vdots \\\\\n",
    "1 & x^N & (x^N)^2 & \\cdots & (x^N)^k \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_k \\end{array} \\right] = \\left[\\begin{array}{c} g(x^1) \\\\ g(x^2) \\\\ \\vdots \\\\ g(x^N) \\end{array} \\right]$$\n",
    "\n",
    "You can do matrix multiplication using the [dot](https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html) function. Write 2 functions for computing the polynomial below\n",
    "\n",
    "* `poly_val` should take a single input value $x$ and a vector of polynomial weights $W$ and compute the single hypothesis value for that input.\n",
    "* `poly_model` should take a matrix $D$ and weight vector $W$ and compute the corresponding vector of hypotheses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_val(x, W):\n",
    "    np.dot(x,W)\n",
    "\n",
    "def poly_model(D, W):\n",
    "    np.dot(D,W)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial cost function and model fitting [3 pts]\n",
    "\n",
    "And for the cost function we can now use\n",
    "\n",
    "$$ E(w|X) = \\frac{1}{2N} \\sum_{t=1}^N [r^t - D^tw]^2$$\n",
    "\n",
    "Here, we compute the hypothesis $g(x)$ for every example using $D^tw$, take the difference with the actual output $r$ and finally square and sum each difference. Note that this is extremely similar to the mean squared error function we used for the linear case, and also that minimizing this error function is actually equivalent to maximizing the log likelihood of the parameter vector $w$ (see equations $4.31$ and $4.32$).\n",
    "\n",
    "Now we have the cost function equation and can again take the partial derivative for each of the weights $w_0$ to $w_k$ and set their value equal to $0$. Solving the resulting system of equations will give the set of weights that minimize the cost function. The weights describing this lowest point of the cost function are the parameters which will produce the line that best fits our dataset.\n",
    "\n",
    "Solving all partial derivate equations for each weight can actually be done with just a couple of matrix operations. Deriving the equation yourself can be a bit involved, but know that the principle is exactly the same as for the linear model computing just $w_0$ and $w_1$. The final equation for weight vector becomes\n",
    "\n",
    "(4.33) $$ w = (D^TD)^{-1}D^Tr $$\n",
    "\n",
    "Numpy has built in functions for [transpose](https://docs.scipy.org/doc/numpy/reference/generated/numpy.transpose.html) and [inverse](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.inv.html). Use them to write the code for the following functions.\n",
    "\n",
    "* `poly_cost` should return the total cost $E$ given $w$, $D$ and $r$\n",
    "* `poly_fit` should return the vector $w$ that bests fits the polynomial relationship between matrix $D$ and vector $r$\n",
    "\n",
    "Using the quadratic matrix $D$ you constructed earlier and this `poly_fit` function, find the best fitting weights for a quadric polynomial on the data and print these weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.,  5.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def poly_cost(W, D, R):\n",
    "    1/2*np.sum(R-(D*w))**2\n",
    "\n",
    "def poly_fit(D, R):\n",
    "    return np.matmul(np.linalg.inv(np.matmul(np.transpose(D), D)),np.matmul(np.transpose(D), R)) \n",
    "\n",
    "poly_fit(D,[9,2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting polynomials [1 pt]\n",
    "\n",
    "Now lets try and figure out what our fitted quadratic polynomial looks like. As the function is not linear, we will need more than just 2 points to actually plot the line. The easiest solution is to create a whole bunch of x-values as samples, compute the corresponding y-values and plot those. With enough samples the line will look smooth, even if it is connected with linear segments.\n",
    "\n",
    "To create these x-values samples, we can use the function [linspace](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html). Then just use the `poly_val` function you wrote earlier and apply it to every x-value to compute the array of y-values. Now just plot the original datapoints as dots and the hypothesis as a line, just as for the linear plot. Don't forget to show your plot at the end.\n",
    "\n",
    "Use these steps to fill in the `poly_plot` function below and show the polynomial function defined by the weights you found for the quadratic polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_plot(W, X, R):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial order [1 pt]\n",
    "\n",
    "You can now create a polynomial fit on the data for a polynomial of any order. The next question then becomes: *What order polynomial fits the data the best?*\n",
    "\n",
    "Using the `create_D_matrix`, `poly_fit` and `poly_plot`, try to fit different order polynomials to the data. Show the plot for the order polynomial you think fits best.\n",
    "\n",
    "Note that the cost function will most likely decrease with each added polynomial term, as there is more flexibility in the model to fit the data points exactly. However, these weights will fit those few data points very well, but might have very extreme values in between points that would not be good predictors for new inputs. Something like an order 20 polynomial might have a very well fitting shape for the existing data points, but looks like it would be strange predictor at some of the possible other points. Try to find a fit that looks visually like it would generalize well to new points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation [2 pt]\n",
    "\n",
    "Another way to answer this same question is to use cross validation. With cross validation you split the data into 2 parts and use one part to fit the model (training set) and the other part to see how well the model fits the remaining data (validation set).\n",
    "\n",
    "Write a function below to split the original dataset into 2 sets according to a given ratio. It is important to randomize your division, as simply using the first half of data for the one set and the second half for the other, might result in a strange distribution. You could use a function like [shuffle](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html) for this purpose.\n",
    "\n",
    "Split the original dataset using a ratio of 0.6 into a training and a validation set. Then for both of these sets, use your old `split_X_R` function to split them into their $X$ and $R$ parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(data, ratio):\n",
    "    np.random.shuffle(data)\n",
    "    training = [data[i] for i in range(round(len(data)*ratio))]\n",
    "    test = [data[i] for i in range(round(len(data)*ratio), len(data))]\n",
    "    return training,test\n",
    "training, test = validation_split(load_file('points.csv'), 0.6)\n",
    "\n",
    "Xtrain,Rtrain = split_X_R(training)\n",
    "Xtest,Rtest = split_X_R(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection [5 pt]\n",
    "\n",
    "With this new split of the data you can just repeatedly fit different order polynomials to the training set and see which produces the lowest cost on the validation set. The set of weights with the lowests cost on the validation set generalizes the best to new data and is thus the best overal fit on the dataset.\n",
    "\n",
    "Write the function `best_poly_fit` below. Try a large range of polynomial orders (like 1 to 50), create the $D$ matrix based on the training set for each order and fit the weights for that polynomial. Then for each of these found weights, also create the D matrix for the validation set and compute the cost using `poly_cost`. Return the set of weights with the lowest cost on the validation set.\n",
    "\n",
    "Run this fitting function with your training and validation sets. Plot the hypothesis function and show the weights that were found. Note that rerunning your validation split code above will result in a different random distribution and thus a slightly different final fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_poly_fit(train_x, train_r, val_x, val_r):\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
