{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: Kaj Meijer\n",
    "\n",
    "### Student ID: 10509534\n",
    "\n",
    "### Group: K\n",
    "\n",
    "Please fill in you name, student ID and group above, and also edit the filename according to the specified format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "For this assignment we are going to build an implementation of the univariate decision tree algorithm that can classify using both discrete and numeric variables. It will be based off the pseudocode in figure 9.3 of *Alpaydin*, so study that first to understand the general idea of the algorithm. Besides figuring out how exactly to do these splits for discrete and numeric variables, this assignment will give us a chance to study the following topics:\n",
    "\n",
    "* Working with a more realistic data set. The Iris dataset is useful to get started, as it just works out of the box, but real world datasets will almost always be a lot messier. Having to do some preprocessing to end up with a usable representation is extremely common.\n",
    "* Working with more complex representations than just data in Nd-arrays. Many things can be captured as just collections of rows and columns, but if you want to build a structure like a tree (or a graph) then objects might be easier to work with. Creating objects in *Python* can be daunting at first if you are unfamiliar with them, but it will actually be easier to work with a complex represenation using these abstractions. They are a very useful tool to add to your arsenal.\n",
    "* Analysing the results of an algorithm. The results you get when you have (correctly) implemented the algorithm might surprise you. Trying to set up hypotheses about why this is the case and what you could do to improve / prevent / fix this, is a key skill in applying machine learning on real problems\n",
    "\n",
    "# Predicting heart disease \n",
    "\n",
    "The data set we will be using are heart disease diagnosis results from 4 different hospitals. The data set can be found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/).\n",
    "\n",
    "Lets start by looking at the [heart-disease.names](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/heart-disease.names) file, which contains a description of the data set. The file also gives an explanation for the values of the different variables, so when our tree is complete we can interpret the decision rules created by the algorithm. \n",
    "\n",
    "Some variables included here, like *#9 cp: chest pain type*, with 4 labels for different types of chest pain, are clearly discrete. And then are also variables like *#12 chol: serum cholestoral in mg/dl*, containing the concentration of cholesterol, an obvious numeric value. The ability to handle both of these types of data is something not many other machine learning algorithms can do effectively, so in theory a decision tree should be perfect for this data.\n",
    "\n",
    "## Taking a first look [1 pt]\n",
    "\n",
    "Start by downloading the 4 `processed.X.data` files and loading them into *Numpy*. For this complete the by now familiar `load_data` function. Running this might give you an error, because not all values could be converted to a `float` or an `int`. Fix this by leaving the datatype as a `str` for now, so we can at least see what the data looks like.\n",
    "\n",
    "For each of the 4 data sets, print the size of the data set and the first row of values. You should now see an unexpected value pop up, which probably indicates a missing value. The file describing the data sets states that missing values are indicated by $-9.0$, but this doesn't seem to be the case. Lets inspect the scope of this problem by writing the `count_missing` function, which should count the number of missing elements for each variable / column of a data set. The function should thus return an array of missing counts, one count for each variable in the data set. Print the results for each of the 4 data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter \n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    txt = np.loadtxt(filename, delimiter=',', dtype=str)\n",
    "    return txt\n",
    "\n",
    "def count_missing(data):\n",
    "    missing = np.zeros(np.shape(data)[1])\n",
    "    \n",
    "    # go through the rows\n",
    "    for row in data:\n",
    "        \n",
    "        # go through the columns\n",
    "        for index in range(len(row)):\n",
    "            \n",
    "            # mark as missing if the value is '?'\n",
    "            if row[index] == \"?\":\n",
    "                missing[index] += 1\n",
    "                \n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleveland = load_data(\"processed.cleveland.data\")\n",
    "hungarian = load_data(\"processed.hungarian.data\")\n",
    "switzerland = load_data(\"processed.switzerland.data\")\n",
    "va = load_data(\"processed.va.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleveland:  (303, 14) ['63.0' '1.0' '1.0' '145.0' '233.0' '1.0' '2.0' '150.0' '0.0' '2.3' '3.0'\n",
      " '0.0' '6.0' '0']\n",
      "Hungarian:  (294, 14) ['28' '1' '2' '130' '132' '0' '2' '185' '0' '0' '?' '?' '?' '0']\n",
      "Switzerland:  (123, 14) ['32' '1' '1' '95' '0' '?' '0' '127' '0' '.7' '1' '?' '?' '1']\n",
      "VA:  (200, 14) ['63' '1' '4' '140' '260' '0' '1' '112' '1' '3' '2' '?' '?' '2']\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleveland: \", np.shape(cleveland), cleveland[0])\n",
    "print(\"Hungarian: \", np.shape(hungarian), hungarian[0])\n",
    "print(\"Switzerland: \", np.shape(switzerland), switzerland[0])\n",
    "print(\"VA: \", np.shape(va), va[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cleveland = count_missing(cleveland)\n",
    "missing_hungarian = count_missing(hungarian)\n",
    "missing_switzerland = count_missing(switzerland)\n",
    "missing_va = count_missing(va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleveland:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4. 2. 0.]\n",
      "Hungarian:  [  0.   0.   0.   1.  23.   8.   1.   1.   1.   0. 190. 291. 266.   0.]\n",
      "Switzerland:  [  0.   0.   0.   2.   0.  75.   1.   1.   1.   6.  17. 118.  52.   0.]\n",
      "VA:  [  0.   0.   0.  56.   7.   7.   0.  53.  53.  56. 102. 198. 166.   0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleveland: \", missing_cleveland)\n",
    "print(\"Hungarian: \", missing_hungarian)\n",
    "print(\"Switzerland: \", missing_switzerland)\n",
    "print(\"VA: \", missing_va)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the data [2 pts]\n",
    "\n",
    "Looking at the results from the previous step, it seems like the sets from some hospitals are more complete than others. There are different approaches you might take to solve this, like replacing the missing values with the average value for that variable, or handling missing values within the algorithm in a seperate way. For now we will take the simplest approach, discarding any rows that contain missing values. This way we only use the complete patient records from each data set. Write the `remove_missing` function, which should remove any rows containing a missing value and return the new 2d-array. Combine all 4 cleaned sets into a single data array and print the shape to see how many patients we are left with.\n",
    "\n",
    "Now we should have about 300 patient records, most of which are from the Cleveland hospital. For each patient we have 14 variables, but we don't know which are discrete and which are numeric yet. We could study the description of each variable, see if can interpret them all and label them that way. However, a good automatic indicator might be the number of unique values for a variable, as you would expect that number to be much higher for numeric variables than for discrete ones. Write the function `unique_vals` which should return an array of the number of unique values for each column a data array. Use that function on the combined data array and print the resulting array of counts.\n",
    "\n",
    "Some values clearly stand out as discrete from these counts, so it should be easy enough to make the distinction. Another thing that might stand out is the fact that the last column, the predicted variable according to the description file, has 5 separate values. Lets inspect how these values are distributed by writing the function `count_occurence`, which should take a single data column and return a dictionary with all possible values of a variable as the keys and their respective counts as the value. Print the resulting counts for the last column of the data array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_missing(data):\n",
    "    \n",
    "    # remove every row that contains at least one '?'\n",
    "    return [row for row in data if \"?\" not in row]\n",
    "\n",
    "def unique_vals(data):\n",
    "    \n",
    "    columns = np.shape(data)[1]\n",
    "    unique = np.zeros((columns))\n",
    "    \n",
    "    for index in range(columns):\n",
    "        \n",
    "        # remove duplicates\n",
    "        unique[index] = len(list(set(data[:,index])))\n",
    "        \n",
    "    return unique\n",
    "\n",
    "def count_occurence(data_column):\n",
    "    \n",
    "    # count the unique values in the column\n",
    "    return Counter(data_column)\n",
    "\n",
    "def count_all_occurences(data):\n",
    "    occurences = []\n",
    "    \n",
    "    # count the occurences for all columns\n",
    "    for index in range(np.shape(data)[1]):\n",
    "        occurences.append(count_occurence(data[:,index]))\n",
    "    \n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_cleveland = remove_missing(cleveland)\n",
    "complete_hungarian = remove_missing(hungarian)\n",
    "complete_switzerland = remove_missing(switzerland)\n",
    "complete_va = remove_missing(va)\n",
    "\n",
    "# combine the data and store as np array\n",
    "complete_data = np.asarray(complete_cleveland+complete_hungarian+complete_switzerland+complete_va)\n",
    "\n",
    "# change the values from strings to floats since we removed the incomplete rows\n",
    "complete_data = complete_data.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleveland:  (297, 14)\n",
      "Hungarian:  (1, 14)\n",
      "Switzerland:  (0,)\n",
      "VA:  (1, 14)\n",
      "Combined data:  (299, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleveland: \", np.shape(complete_cleveland))\n",
    "print(\"Hungarian: \", np.shape(complete_hungarian))\n",
    "print(\"Switzerland: \", np.shape(complete_switzerland))\n",
    "print(\"VA: \", np.shape(complete_va))\n",
    "print(\"Combined data: \", np.shape(complete_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = unique_vals(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values:  [ 41.   2.   4.  50. 153.   2.   3.  92.   2.  40.   3.   4.   3.   5.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values: \", unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = count_occurence(complete_data[:,0])\n",
    "occs = count_all_occurences(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Counter({58.0: 18, 57.0: 17, 54.0: 16, 59.0: 14, 56.0: 12, 60.0: 12, 51.0: 12, 62.0: 11, 44.0: 11, 52.0: 11, 41.0: 10, 64.0: 10, 63.0: 9, 67.0: 9, 42.0: 8, 55.0: 8, 61.0: 8, 65.0: 8, 45.0: 8, 53.0: 7, 48.0: 7, 50.0: 7, 66.0: 7, 43.0: 7, 46.0: 7, 47.0: 6, 49.0: 5, 39.0: 4, 68.0: 4, 35.0: 4, 70.0: 4, 40.0: 3, 69.0: 3, 71.0: 3, 37.0: 2, 34.0: 2, 29.0: 1, 77.0: 1, 38.0: 1, 74.0: 1, 76.0: 1})\n",
      "[Counter({58.0: 18, 57.0: 17, 54.0: 16, 59.0: 14, 56.0: 12, 60.0: 12, 51.0: 12, 62.0: 11, 44.0: 11, 52.0: 11, 41.0: 10, 64.0: 10, 63.0: 9, 67.0: 9, 42.0: 8, 55.0: 8, 61.0: 8, 65.0: 8, 45.0: 8, 53.0: 7, 48.0: 7, 50.0: 7, 66.0: 7, 43.0: 7, 46.0: 7, 47.0: 6, 49.0: 5, 39.0: 4, 68.0: 4, 35.0: 4, 70.0: 4, 40.0: 3, 69.0: 3, 71.0: 3, 37.0: 2, 34.0: 2, 29.0: 1, 77.0: 1, 38.0: 1, 74.0: 1, 76.0: 1}), Counter({1.0: 203, 0.0: 96}), Counter({4.0: 144, 3.0: 83, 2.0: 49, 1.0: 23}), Counter({120.0: 38, 130.0: 36, 140.0: 32, 110.0: 19, 150.0: 18, 160.0: 11, 125.0: 10, 128.0: 10, 138.0: 10, 112.0: 9, 132.0: 7, 118.0: 7, 135.0: 6, 124.0: 6, 108.0: 6, 145.0: 5, 134.0: 5, 152.0: 5, 170.0: 4, 122.0: 4, 100.0: 4, 105.0: 3, 142.0: 3, 180.0: 3, 115.0: 3, 126.0: 3, 136.0: 3, 94.0: 2, 102.0: 2, 148.0: 2, 178.0: 2, 144.0: 2, 146.0: 2, 172.0: 1, 117.0: 1, 155.0: 1, 104.0: 1, 200.0: 1, 165.0: 1, 101.0: 1, 174.0: 1, 158.0: 1, 192.0: 1, 129.0: 1, 123.0: 1, 106.0: 1, 156.0: 1, 154.0: 1, 114.0: 1, 164.0: 1}), Counter({234.0: 6, 197.0: 6, 204.0: 5, 254.0: 5, 226.0: 5, 212.0: 5, 269.0: 5, 233.0: 4, 239.0: 4, 211.0: 4, 177.0: 4, 243.0: 4, 282.0: 4, 240.0: 4, 229.0: 3, 250.0: 3, 236.0: 3, 203.0: 3, 256.0: 3, 263.0: 3, 199.0: 3, 283.0: 3, 219.0: 3, 230.0: 3, 231.0: 3, 258.0: 3, 245.0: 3, 274.0: 3, 201.0: 3, 303.0: 3, 309.0: 3, 249.0: 3, 288.0: 3, 246.0: 3, 244.0: 3, 286.0: 2, 268.0: 2, 192.0: 2, 294.0: 2, 275.0: 2, 266.0: 2, 206.0: 2, 335.0: 2, 225.0: 2, 302.0: 2, 330.0: 2, 198.0: 2, 253.0: 2, 273.0: 2, 213.0: 2, 305.0: 2, 304.0: 2, 188.0: 2, 232.0: 2, 267.0: 2, 248.0: 2, 308.0: 2, 270.0: 2, 208.0: 2, 264.0: 2, 325.0: 2, 235.0: 2, 255.0: 2, 222.0: 2, 260.0: 2, 265.0: 2, 220.0: 2, 209.0: 2, 227.0: 2, 261.0: 2, 221.0: 2, 205.0: 2, 289.0: 2, 318.0: 2, 298.0: 2, 299.0: 2, 277.0: 2, 214.0: 2, 207.0: 2, 315.0: 2, 196.0: 2, 228.0: 2, 193.0: 2, 271.0: 2, 149.0: 2, 295.0: 2, 218.0: 2, 223.0: 2, 354.0: 1, 168.0: 1, 284.0: 1, 224.0: 1, 340.0: 1, 247.0: 1, 167.0: 1, 276.0: 1, 353.0: 1, 175.0: 1, 417.0: 1, 290.0: 1, 172.0: 1, 216.0: 1, 185.0: 1, 326.0: 1, 360.0: 1, 321.0: 1, 257.0: 1, 164.0: 1, 141.0: 1, 252.0: 1, 182.0: 1, 307.0: 1, 186.0: 1, 341.0: 1, 183.0: 1, 407.0: 1, 217.0: 1, 174.0: 1, 281.0: 1, 564.0: 1, 322.0: 1, 300.0: 1, 293.0: 1, 160.0: 1, 394.0: 1, 184.0: 1, 409.0: 1, 195.0: 1, 126.0: 1, 313.0: 1, 259.0: 1, 200.0: 1, 262.0: 1, 215.0: 1, 210.0: 1, 327.0: 1, 306.0: 1, 178.0: 1, 237.0: 1, 242.0: 1, 319.0: 1, 166.0: 1, 180.0: 1, 311.0: 1, 278.0: 1, 342.0: 1, 169.0: 1, 187.0: 1, 157.0: 1, 176.0: 1, 241.0: 1, 131.0: 1, 100.0: 1}), Counter({0.0: 256, 1.0: 43}), Counter({0.0: 149, 2.0: 146, 1.0: 4}), Counter({162.0: 11, 160.0: 9, 163.0: 9, 152.0: 8, 150.0: 7, 172.0: 7, 132.0: 7, 125.0: 7, 142.0: 6, 173.0: 6, 144.0: 6, 158.0: 6, 161.0: 6, 140.0: 6, 143.0: 6, 178.0: 5, 147.0: 5, 174.0: 5, 168.0: 5, 179.0: 5, 157.0: 5, 169.0: 5, 165.0: 5, 170.0: 5, 182.0: 5, 156.0: 5, 154.0: 5, 155.0: 4, 171.0: 4, 151.0: 4, 120.0: 4, 131.0: 4, 145.0: 4, 146.0: 4, 159.0: 4, 130.0: 4, 126.0: 4, 122.0: 4, 148.0: 3, 153.0: 3, 114.0: 3, 141.0: 3, 111.0: 3, 175.0: 3, 166.0: 3, 138.0: 3, 105.0: 3, 108.0: 2, 139.0: 2, 112.0: 2, 123.0: 2, 109.0: 2, 180.0: 2, 149.0: 2, 186.0: 2, 136.0: 2, 133.0: 2, 103.0: 2, 164.0: 2, 96.0: 2, 115.0: 2, 116.0: 2, 181.0: 2, 129.0: 1, 187.0: 1, 137.0: 1, 128.0: 1, 188.0: 1, 113.0: 1, 99.0: 1, 177.0: 1, 185.0: 1, 190.0: 1, 97.0: 1, 127.0: 1, 202.0: 1, 184.0: 1, 124.0: 1, 88.0: 1, 194.0: 1, 195.0: 1, 106.0: 1, 167.0: 1, 95.0: 1, 192.0: 1, 117.0: 1, 121.0: 1, 71.0: 1, 118.0: 1, 134.0: 1, 90.0: 1, 98.0: 1}), Counter({0.0: 200, 1.0: 99}), Counter({0.0: 96, 1.2: 17, 0.6: 14, 1.4: 13, 0.8: 13, 1.0: 13, 0.2: 12, 1.6: 11, 1.8: 10, 2.0: 9, 0.4: 8, 1.5: 7, 2.6: 6, 2.8: 6, 0.1: 6, 0.5: 5, 3.0: 5, 1.9: 5, 3.6: 4, 2.2: 4, 2.4: 3, 3.4: 3, 4.0: 3, 0.9: 3, 0.3: 3, 2.3: 2, 3.2: 2, 2.5: 2, 4.2: 2, 1.1: 2, 3.5: 1, 3.1: 1, 1.3: 1, 6.2: 1, 5.6: 1, 2.9: 1, 2.1: 1, 3.8: 1, 0.7: 1, 4.4: 1}), Counter({2.0: 139, 1.0: 139, 3.0: 21}), Counter({0.0: 176, 1.0: 65, 2.0: 38, 3.0: 20}), Counter({3.0: 164, 7.0: 117, 6.0: 18}), Counter({0.0: 160, 1.0: 56, 2.0: 35, 3.0: 35, 4.0: 13})]\n"
     ]
    }
   ],
   "source": [
    "print(len(occ), occ)\n",
    "print(occs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 different splits [2 pts]\n",
    "\n",
    "Now that we have a bit of an idea of what the data looks like, we can start separating out the parts. First complete another familiar function, the `validation_split`, as we will need to use a portion of the data to validate. Split the combined data set using a ratio of $0.7$.\n",
    "\n",
    "Next write a function to split the data into **x** values and the predicted **y** classes, called `x_y_split`. For the **y** values you probably noticed after the previous step that there are many more $0$ labels than any of the other 4 labels. In the description file it says *Value 0: < 50% diameter narrowing*, but it does not explain all the other values. It might be logical to assume these to be different degrees of narrowing, so $0$ would mean no disease and higher values would mean different levels of disease present. Because the distribution of the different values is so skewed, for now we will just focus on classifying the difference between a value of $0$ for the narrowing and any of the higher values. This means the **y** vector should just contain boolean value which is `True` if there is more than $50\\%$ narrowing and `False` otherwise. The **x** portion of the data should remain a 2d-array of strings, as some are still discrete and some numeric. Split the training and validation sets into **x** and **y** parts\n",
    "\n",
    "Finally we will split the **x** arrays into discrete and numeric parts, using `discrete_numeric_split`. Use the `unique_vals` function from earlier and select the variables that have no more than the `theshold` argument number of unique values as discrete variables and all others as numeric variables. The discrete variables are all whole numbers, so the resulting 2d-array should be of type `int` and the numeric variables array should of type `float`. You can return both together as a [tuple](https://docs.python.org/3.3/tutorial/datastructures.html#tuples-and-sequences) (also works for the other split functions) and then use tuple unpacking to easily store them into separate variables. Apply the function `discrete_numeric_split` to the **x** 2d-arrays of the training and validation sets using a `threshold` value of $10$.\n",
    "\n",
    "The training data should now consist of 3 different parts:\n",
    "\n",
    "1. A 2d-array of integers containing the discrete variables for each patient.\n",
    "2. A 2d-array of floats containing the numeric variables for each patient.\n",
    "3. A 1d-array of booleans containing the disease diagnosis for each patient.\n",
    "\n",
    "Print all 3 of these variables and make sure they look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_split(data, ratio):\n",
    "    real_ratio = round(len(data)*ratio)\n",
    "    return np.asarray(data[:real_ratio]), np.asarray(data[real_ratio:])\n",
    "    \n",
    "\n",
    "def x_y_split(data):\n",
    "    columns = np.shape(data)[1]-1\n",
    "    x = data[:,:columns]\n",
    "    y = data[:,columns]\n",
    "    np.minimum(y,1,y)\n",
    "    return x, y\n",
    "\n",
    "def discrete_numeric_split(data, thres=10):\n",
    "    numeric = []\n",
    "    discrete = []\n",
    "    unique_values = unique_vals(data)\n",
    "    \n",
    "    for index in range(len(unique_values)):\n",
    "        if unique_values[index] >= thres:\n",
    "            numeric += [list(data[:, index])]\n",
    "        else :\n",
    "            discrete += [list(data[:, index])]\n",
    "    \n",
    "    return np.asarray(discrete), np.asarray(numeric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = validation_split(complete_data, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 90 0.6989966555183946\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(validation), len(train)/(len(train)+len(validation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = x_y_split(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 13) (209,)\n",
      "[[63.  1.  1. ...  3.  0.  6.]\n",
      " [67.  1.  4. ...  2.  3.  3.]\n",
      " [67.  1.  4. ...  2.  2.  7.]\n",
      " ...\n",
      " [62.  0.  4. ...  2.  0.  3.]\n",
      " [37.  0.  3. ...  1.  0.  3.]\n",
      " [38.  1.  1. ...  2.  0.  7.]]\n",
      "[0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_x), np.shape(train_y))\n",
    "print(train_x)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx, nx = discrete_numeric_split(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [[1. 1. 1. ... 0. 0. 1.]\n",
      " [1. 4. 4. ... 4. 3. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [3. 2. 2. ... 2. 1. 2.]\n",
      " [0. 3. 2. ... 0. 0. 0.]\n",
      " [6. 3. 7. ... 3. 3. 7.]]\n",
      "5 [[ 63.   67.   67.  ...  62.   37.   38. ]\n",
      " [145.  160.  120.  ... 150.  120.  120. ]\n",
      " [233.  286.  229.  ... 244.  215.  231. ]\n",
      " [150.  108.  129.  ... 154.  170.  182. ]\n",
      " [  2.3   1.5   2.6 ...   1.4   0.    3.8]]\n"
     ]
    }
   ],
   "source": [
    "print(len(dx), dx)\n",
    "print(len(nx),nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recombining discrete and numeric [1 pt]\n",
    "\n",
    "We now have 2 arrays representing the patient data, however for our algorithm it would be a lot more convenient to have the discrete and numeric combined in a single row containing all the variables. Since they are different types (integers and floats), they cannot be stored together in the same array, but we can combine them in an object. So next we will write a class `DataRow`, which will do exactly that: store the discrete and numeric data of a patient together in an object.\n",
    "\n",
    "This class will be a very basic example of a class as used in *object oriented* programming, a container for some variables and some functions to access those variables. If you are completely unfamiliar with *OO* in Python, read a short introduction [here](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/), until the section on *Instance Attributes and Methods*.\n",
    "\n",
    "Start by writing the `__init__` function for the class, to store the discrete and numeric values in the object. Now write both `get_X` functions, which should retrieve the value of that type stored at that index. Finally write the `size_X` functions, which should return the number of elements of that type that are stored in the object.\n",
    "\n",
    "This completes your first class. Now lets put the class to use to create some objects and fill those with the data. Write the function `create_rows`, which takes a 2d-array of discrete values and a 2d-array of numeric values, and combines each row of both together in a `DataRow` object. The function should return an array of `DataRow` instances, with one instance for each row in the two original arrays.\n",
    "\n",
    "Use this `create_rows` function to combine both the discrete and numeric training data, and to combine the validation data in the same way. To show your function works, take the first `DataRow` from the training data and print one of the numeric values it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRow(object):\n",
    "    def __init__(self, discrete, numeric):\n",
    "        self.discrete = discrete\n",
    "        self.numeric = numeric\n",
    "        self.sd = len(discrete)\n",
    "        self.sn = len(numeric)\n",
    "        \n",
    "    def get_discrete(self, index):\n",
    "        # get the discrete value of the index is valid\n",
    "        if index < self.size_discrete():\n",
    "            return self.discrete[index]\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def get_numeric(self, index):\n",
    "        # get the numeric value if the index is valid\n",
    "        if index < self.size_numeric():\n",
    "            return self.numeric[index]\n",
    "        else:\n",
    "            return -1\n",
    "        \n",
    "    def size_discrete(self):\n",
    "        # return the number of discrete variables\n",
    "        return self.sd\n",
    "    \n",
    "    def size_numeric(self):\n",
    "        # return the number of numeric variables\n",
    "        return self.sn\n",
    "    \n",
    "def create_rows(discrete, numeric):\n",
    "    \n",
    "    # initialize the array\n",
    "    dr = []\n",
    "    \n",
    "    # add the rows that are created with the discrete and numeric values\n",
    "    for index in range(len(discrete[0])):\n",
    "        dr.append(DataRow(discrete[:,index], numeric[:,index]))\n",
    "        \n",
    "    # return as np array\n",
    "    return np.asarray(dr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the rows\n",
    "train_data_rows = create_rows(dx, nx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for me\n",
    "# print(len(data_rows), data_rows[0].size_discrete(), data_rows[0].size_numeric())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy [3 pts]\n",
    "\n",
    "There quite a few different definitions of what entropy is; all of them relate to the notion of chaos / order in a system, but the exact definition strongly depends on the context in which the term is used. Most commonly the term refers to thermodynamic entropy, where it is the describes the number possible configurations a thermodynamic system can have in a specific state. This is related the idea of a universal entropy, as used in Asimov's classic short story [The Last Question](http://multivax.com/last_question.html). For decision trees we need the information theoretic entropy, or Shannon entropy, which says something about the amount of information contained in a distribution of data. The more ordered or one-sided the distribution is, the less bits we would need on average to express the exact distribution.\n",
    "\n",
    "We will use this measure of entropy to compare the results of decision tree splits to see which provides the biggest increase in consistency of distributions. For the heart disease problem there are only 2 class labels we are considering, `True` if *vascular narrowing >= 50% diameter* and `False` otherwise. For a 2 class problem, the entropy is defined as:\n",
    "\n",
    "(9.4) $$\\phi(p) = −p\\ log_2(p) − (1 − p)\\ log_2(1 − p)$$\n",
    "\n",
    "where $p$ is the ratio between between the labels for class 1 and class 2. First write a `ratio` function to compute this value. The function should, given a list of boolean values as class labels, return the ratio of `True` labels in the list, e.g. $1.0$ would indicate the list only contained `True`.\n",
    "\n",
    "The computation of $0\\ log_2(0)$ will (correctly) result in a math error, but it could also just be defined as having the value $0$ (as it is multiplied by $0$). Write the function `entropy_sub` to compute the value of this log product, making sure to return $0$ in the case that $p$ is $0$ instead of an error. Now combine `ratio` and `entropy_sub` to compute the `entropy` of a list of boolean class labels.\n",
    "\n",
    "When a set of labels is split on a variable $m$, 2 or more new lists are created, each with there own entropy. Combining the resulting entropies from a split into $s$ new sets is a simple weighted sum:\n",
    "\n",
    "(9.8) $$I_m = \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "where $N_j$ is the size of the $j^{th}$ split distribution, $p_j$ is the ratio of the class labels for that same $j^{th}$ distribution and $N$ is the size of the distribution before the split. Write the function `split_entropy` to compute this this value for list of label lists and a value for `N`.\n",
    "\n",
    "There are serveral metrics we can use to asses how good a split is in a decision tree. For this implemenation we will use the *Information Gain*, which is defined as the entropy of the original distribution $\\phi(p)$ minus the entropy of the split distribtution $I_m$, resulting from the split on variable $m$.\n",
    "\n",
    "$$IG_m = \\phi(p) - \\sum_{j=1}^s \\frac{N_j}{N} \\phi(p_j)$$\n",
    "\n",
    "Information Gain therefore measures how much the entropy changes from making a specific split, i.e. the relative gain in predicitablity of the data by making a specific distribution of labels. The IG thus depends on 2 things; the original list of labels and how these labels are divided into new distributions by the split. This last part will be specified by a list of lists of indices, where the first list contains all the indices of the labels that belong to the first part of the split, the second list contains the indices of the labels for the second part and so on. The reason for this format will become clearer as you write the actual code for the splitting function, for now just write the `information_gain` using your earlier functions `entropy` and `split_entropy` and assume `indices` is list containing a list of indices for each part of the split.\n",
    "\n",
    "Finally write a `plot_entropy` function to check the entropy function works correctly. This function should create many different lists of boolean labels of length `N` and compute ratio and entropy for each of these lists. Note that for list of boolean of length $N$, there are only $N+1$ different possible ratios of labels you need to create. The x-axis of your plot should the ratios and the y-axis their resulting entropies, which should produce a graph like Figure 9.2 in *Alpaydin*. Show this plot at the end of your code and make sure it looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ratio(labels):\n",
    "    length = len(labels)\n",
    "    if length:\n",
    "        return sum(labels)/len(labels)\n",
    "    return 0\n",
    "\n",
    "def entropy_sub(p):\n",
    "    if p == 0:\n",
    "        return 0\n",
    "    return p*math.log(p,2)\n",
    "\n",
    "def entropy(labels):\n",
    "    p = ratio(labels)\n",
    "    return -entropy_sub(p)-entropy_sub(1-p)\n",
    "\n",
    "def split_entropy(labels, N):\n",
    "    N_labels = len(labels)\n",
    "    return (N_labels/N)*entropy(labels)\n",
    "\n",
    "def information_gain(l, indices):\n",
    "    \n",
    "    labels = l.copy()\n",
    "    # literal indices of the full list \n",
    "    # indices = [0, 4, 6] -> y[0], y[4], y[6]\n",
    "    \n",
    "    # get the number of labels\n",
    "    length = len(labels)\n",
    "    \n",
    "    e = entropy(labels)\n",
    "    se = 0\n",
    "    \n",
    "    for sub_indices in indices:\n",
    "        \n",
    "        # get the labels of the indices\n",
    "        sub_labels = [labels[index] for index in sub_indices if index < length] \n",
    "        \n",
    "        se += split_entropy(sub_labels,length)\n",
    "        \n",
    "        del sub_labels\n",
    "        \n",
    "    return e-se\n",
    "\n",
    "# m is the number of different lists\n",
    "def plot_entropy(N, M):\n",
    "    \n",
    "    x = np.zeros((M))\n",
    "    y = np.zeros((M))\n",
    "    \n",
    "    # for a number of times\n",
    "    for i in range(M):\n",
    "        \n",
    "        # random split\n",
    "        split = np.random.randint(N)\n",
    "        \n",
    "        # create list\n",
    "        random_list = [1 for x in range(split)] + [0 for x in range(N-split)]\n",
    "        \n",
    "        # calculate the ratio and entropy of the list\n",
    "        x[i] = ratio(random_list)\n",
    "        y[i] = entropy(random_list)\n",
    "    \n",
    "    plt.plot(x, y, \"r.\")\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9940302114769565 0.8954930986575714\n"
     ]
    }
   ],
   "source": [
    "# tests for me to check if the code worked\n",
    "e = entropy(train_y)\n",
    "ig = information_gain(train_y, [[11, 22, 33, 44, 55, 66, 77, 88, 99, 111, 122, 133, 144, 155, 166, 177, 188, 199],[2, 3, 4]])\n",
    "print(e,ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_entropy(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree class\n",
    "\n",
    "With the data turned into a usable format and combined, and the functions to measure entropy and IG ready, we can start building the actual decision tree. Classes are also a great way to represent trees, where each object is a node in the tree, which can itself contain several branches to other nodes. So the class only has the define the attributes and functions for one node, but repeating this structure can create complex trees. For our decision tree, we will have 2 types of nodes:\n",
    "\n",
    "1. DiscreteTree nodes, split on the value a discrete variable\n",
    "2. NumericTree nodes, split on the value of numeric variable\n",
    "\n",
    "For both of these the splits work slightly differently, but the nodes have many attributes in common too. In order to avoid repeating this in 2 classes, we will use *inheritance*, a very common object-oriented technique to solve this type of problem. Here we will define a parent class `DecisionTree` that will contain all the common elements for `DiscreteTree` and `NumericTree`. If you want to learn more about inheritance, you can read the section on *Inheritance* from [Jeff Knupp's introduction](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/).\n",
    "\n",
    "The `DataRow` class created earlier was a straightforward class example, but here we will use some less intuitive Python to actually create a general `DecisionTree` node and then swap it out for a specific `DiscreteTree` or `NumericTree` instance based on the split results. This will actually simplify building a tree containing both numeric and discrete splits a lot. The `DecisionTree` class will thus not only hold all the common functions used for both the `DiscreteTree` and `NumericTree` nodes, but we will also use its `__init__` method as generic constructor for either specific type of node. The code to do this swap has already been provided, so you will only need to focus on writing the split methods for the `DiscreteTree` and `NumericTree`.\n",
    "\n",
    "So, for now, nothing actually needs to be added in this class. There are 3 functions here that still need to be filled in: `create_subtrees`, `classify` and `validate`, but we will come back to those after the actual split function has been written, because logically, splitting the data needs to be implemented before these functions make any sense. Start by reading the code that has been provided here, so you have sense of the general structure, and most importantly, what the attributes are for each `DecisionTree` node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    def __init__(self, data, labels, tree_type=0, thres=0.1):\n",
    "        \"\"\" Creates a Decision Tree, based on the following arguments:\n",
    "                data - An array of DataRow objects, each instance containing\n",
    "                        the discrete and numeric data for one patient\n",
    "                labels - An array of boolean class labels, each corresponding to a\n",
    "                        DataRow instance of a patient at the same index. \n",
    "                tree_type - 0: create the Tree with the highest IG every node \n",
    "                            1: create DiscreteTrees only\n",
    "                            2: create NumericTrees only\n",
    "                thres - The cutoff value for IG, to stop splitting the tree.\n",
    "                        Below this value the node becomes a leaf node and no\n",
    "                        further splits are made.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        \n",
    "        # Store the basic attributes for any DecisionTree\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tree_type = tree_type\n",
    "        self.thres = thres\n",
    "        \n",
    "        # Compute the current ratio of labels and assign this node the most common label\n",
    "        self.ratio = ratio(self.labels)\n",
    "        self.label = self.ratio >= 0.5\n",
    "        \n",
    "        if self.tree_type == 1:\n",
    "            # Convert this DecisionTree to a DiscreteTree and perform the split\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            self.convert_tree(discr_tree)\n",
    "        elif self.tree_type == 2:\n",
    "            # Convert this DecisionTree to a NumericTree and perform the split\n",
    "            numer_tree = NumericTree(self)\n",
    "            self.convert_tree(numer_tree)\n",
    "        else:\n",
    "            # Create a DiscreteTree and NumericTree, passing all the stored attributes\n",
    "            # as an argument, and compute the best possible split for each\n",
    "            discr_tree = DiscreteTree(self)\n",
    "            numer_tree = NumericTree(self)\n",
    "            \n",
    "            # Based on the results of the split computations, replace this generic\n",
    "            # DecisionTree node with either a DiscreteTree or a NumericTree node\n",
    "            if discr_tree.info_gain > numer_tree.info_gain:\n",
    "                self.convert_tree(discr_tree)\n",
    "            else:\n",
    "                self.convert_tree(numer_tree)\n",
    "        \n",
    "        # Create an empty dictionary to contain the (possible) branches from this node,\n",
    "        # where the values should be new DecisionTree nodes, or None if not present\n",
    "        self.branches = defaultdict(lambda: None)\n",
    "        \n",
    "        # Check if this split produced a high enough Information Gain to actually create\n",
    "        # the resulting branches with new split nodes below it, else this is a leaf node\n",
    "        self.leaf = self.info_gain < self.thres\n",
    "        if not self.leaf:\n",
    "            self.create_subtrees()\n",
    "    \n",
    "    def store_split_values(self, var_index, var_values, indices, info_gain):\n",
    "        \"\"\" Stores the values of the passed parameters as object attributes. Is intended\n",
    "            to store the results of a split computation for either a DiscreteTree or a\n",
    "            NumericTree. The stored attributes are:\n",
    "                var_index - The DataRow index of the variable on which the split was\n",
    "                    based.\n",
    "                var_values - A list of the possible values that this split variable can\n",
    "                    take, each corresponding to a different branch in the DecisionTree\n",
    "                indices - A list of index lists, with each list containing the indices\n",
    "                    defining a subset of the current data and label attributes, as\n",
    "                    computed by the split. The order of these subsets should match the\n",
    "                    order of the corresponding var_values used to define the branches\n",
    "                    of the split.\n",
    "                info_gain - Information Gain computed for this split\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.var_index = var_index\n",
    "        self.var_values = var_values\n",
    "        self.indices = indices\n",
    "        self.info_gain = info_gain\n",
    "    \n",
    "    def convert_tree(self, new_tree):\n",
    "        \"\"\" Converts this object to the tree passed as the new_tree parameter.\n",
    "            All attributes from the new_tree are transfered.\n",
    "                new_tree - Either a DiscreteTree or a NumericTree instance, to which\n",
    "                            this object is converted\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__class__ = new_tree.__class__\n",
    "        self.__dict__ = new_tree.__dict__\n",
    "    \n",
    "    def create_subtrees(self):\n",
    "        \"\"\" Creates the different subsets of the current data and labels, and makes a\n",
    "            a new DecisionTree node for each such subset, based on the indices attribute\n",
    "            stored after the computed split. These new DecisionTrees are stored in the \n",
    "            branches attribute, a dictionary mapping the value of a variable from the\n",
    "            split to the new DecisionTree created by selecting that value for the split.\"\"\"\n",
    "\n",
    "        # loop over the values of the variable\n",
    "        for value_index in range(len(self.var_values)):\n",
    "            \n",
    "            # (re)set the lists\n",
    "            sub_data = []\n",
    "            sub_labels = []\n",
    "            \n",
    "            # loop over the indices of that variable value and build the lists\n",
    "            for index in self.indices[value_index]:\n",
    "                sub_data.append(self.data[index]) \n",
    "                sub_labels.append(self.labels[index])\n",
    "        \n",
    "            # create the branch\n",
    "            self.branches[self.var_values[value_index]] = DecisionTree(sub_data, sub_labels, self.tree_type, self.thres)\n",
    "        \n",
    "        \n",
    "    def classify(self, row):\n",
    "        \"\"\" Traverses the DecisionTree based on the values stored in the data row and\n",
    "            returns the most common label in the resulting leaf node.\n",
    "                row - The DataRow object containing the values that are being\n",
    "                        classified\"\"\"\n",
    "        \n",
    "        # get the subtree or None\n",
    "        subtree = self.get_subtree(row)\n",
    "        \n",
    "        # classify the row for the subtree\n",
    "        if subtree:\n",
    "            return subtree.classify(row)\n",
    "        \n",
    "        # use this tree for the classification\n",
    "        else:\n",
    "            \n",
    "            # this is the leaf node, check is the 1 label is more common\n",
    "            if sum(self.labels)/len(self.labels) >= 0.5:\n",
    "                return 1.0\n",
    "\n",
    "            # the 0 label is more common\n",
    "            else:\n",
    "                return 0.0\n",
    "    \n",
    "    def validate(self, data, labels):\n",
    "        \"\"\" Classifies all the DataRow instances in data and compares the outcome to \n",
    "            the provided labels. Returns the percentage of elements that was classified\n",
    "            correctly.\n",
    "                data - List of DataRow instances to be classified.\n",
    "                labels - List of boolean labels each belonging to a DataRow instances at\n",
    "                    the same index\"\"\"\n",
    "        \n",
    "        total = len(data)\n",
    "        correct = 0\n",
    "        \n",
    "        # loop over all lables\n",
    "        for index in range(total):\n",
    "            \n",
    "            # get the label of the current index\n",
    "            label = self.classify(data[index]) \n",
    "            \n",
    "            # match the label\n",
    "            if label == labels[index]:\n",
    "                \n",
    "                # mark as correct\n",
    "                correct += 1\n",
    "                \n",
    "        return 100*correct/total \n",
    "        \n",
    "        \n",
    "    def split(self):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_subtree(self, instance):\n",
    "        \"\"\" Must be implemented by the subclass based on the specific type of split performed.\n",
    "            The function here is only to ensure it is implemented, and should not be modified.\"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete split [3 pts]\n",
    "\n",
    "Now we can start actually writing the `split` function for the `DiscreteTree`. This function should, for every discrete variable in the data, try to create a split based on that variable and compute the *Information Gain* of the resulting split. Note that if, for example, there are 3 different values a discrete variable can take, this means there will be 3 subsets as a result of the split, each corresponding to one of the values for the variable. Start by selecting out the \"column\" for the specific variable, i.e. a list containing all the values for that variable, retrieved from the `DataRow` instances. Use that to create a list of indices for each of the different subsets. Using the lists of indices and the current set of labels, computing the *IG* should be pretty easy with the functions you already wrote. The variable which results in the highest *IG* will become the actual split for this node.\n",
    "\n",
    "Once the best variable for the split has been determined, the results of the split need to be stored in the instance, so they can be used to build the rest of the tree. The following attributes should be stored:\n",
    "\n",
    "1. The index of the discrete variable based on which the split was done\n",
    "2. The list of unique values for which the split was done.\n",
    "3. The list of lists with the indices indicating the different subsets of the data that result from the split. The order of these lists should match to the order of the values in the previous point, each corresponding to one branch of the split.\n",
    "4. The Information Gain resulting from the split.\n",
    "\n",
    "These attributes can then be used to build the rest of the tree. Some version of these values are also needed for the `NumericTree` splits, so you can use the general function `store_split_values` from the `DecisionTree` class to store them in the instance.\n",
    "\n",
    "## Creating the subtrees [1 pt]\n",
    "\n",
    "Now that we have a function to split data for a single node, actually turning this into a complete tree should not be too complicated. The idea of the `DecisionTree` is to be able to take the result of the splits and split those further again. It is this pattern that creates a tree-structure, where you expect the classification accuracy to increase with every layer you add to the tree.\n",
    "\n",
    "How many layers you add, i.e. how many splits you keep performing on the data, will obviously affect the results of classification. You could keep going until you seperate out every single row into a seperate leaf node of the tree, but this is very likely to overfit the data. There are quite a few strategies for how to prune the tree, i.e. limit the number of nodes to not overfit. The simplest of these is just to stop splitting when the *Information Gain* of a split drops below a certain threshold.\n",
    "\n",
    "This is exactly what is already implemented as the last step in the `__init__` function of `DecisionTree`. Here the computed *Information Gain* is compared to the threshold and if the gain is too low, the node is labeled as a leaf node. If the node is not a leaf node, then further splits should be attemped and the `create_subtrees` function is called to populate the branches of this node with new subset `DecisionTrees`.\n",
    "\n",
    "Now lets write that `create_subtrees` function. Start by using the indices attribute stored from the split result with the `store_split_values` function to create the actual data subsets. For each list with subset indices, slice out the `DataRows` from the data attribute belonging to that part of the split. Then repeat this for the labels, to slice out the labels corresponding to those same `DataRows`. This will be the data and label values with which you can now create a new `DecisionTree` for that slice result. You can repeat this to a create new `DecisionTree` for each split result. \n",
    "\n",
    "In order to actually find the correct `DecisionTree` node when we are classifying, each new `DecisionTree` should be stored in the `branches` dictionary of the node. Here the *key* should be the value of the discrete variable for that split (also stored using `store_split_values`) and the *value* should be the new `DecisionTree` resulting from that choice in the split. When this dictionary is complete, each key-value pair will represent a different branch of the tree at that split.\n",
    "\n",
    "## Retrieving the subtrees, classifying and validating [2 pts]\n",
    "\n",
    "As mentioned in the previous section, the tree building part of the algorithm is actually complete now. This would be a good time to try and visualize what all the code you just wrote actually does. Take out a pen and paper and try to draw the structure that would be built if you created a new `DecisionTree` using the training data, consisting of the `DataRows` and corresponding labels. No need to do the actual entropy math on paper, just try and sketch what objects would be created and how they would relate to each other. If you are have trouble visualizing this for such a large dataset, take a look at the small example tree in *Figure 9.6* in Alpaydin.\n",
    "\n",
    "With this whole structure built, actually classifying a new `DataRow` is pretty easy. First write the function `get_subtree` in `DiscreteTree`, which should return the correct `DecisionTree` one branch down, based on the split made in that node and the values in the `row` object. Remember that as part of the split you also stored the `DataRow` index of the variable used in the split with the `store_split_values` function. It is possible a discrete value is being classified that was not present in the training set at that point in the tree, in that case the function should return `None`.\n",
    "\n",
    "Now write the `classify` function in `DecisionTree` which should classify a `DataRow` using the built tree. There are only a couple of options when classifying on a node\n",
    "\n",
    "1. The node is a leaf node, in which case the classification will be the most common label of that node\n",
    "2. The node does not have a valid subtree for the splitted value, so the classification will also be the node label\n",
    "3. The node does have subtree for the splitted value, in which case you can recursively continue classifying on the subtree\n",
    "\n",
    "Finally, write a `validate` function in `DecisionTree` which should take a validation set of `DataRows` and corresponding labels, and return the percentage which was classified correctly. Create a `DecisionTree` using the training data, with `tree_type` set to $1$ (currently we can only do discrete splits) and print the results when validating with the validation set created earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one entry in the column\n",
    "class ColumnEntry(object):\n",
    "    def __init__(self, row_index, value):\n",
    "        self.index = row_index\n",
    "        self.value = value\n",
    "\n",
    "# one data column\n",
    "class DataColumn(object):\n",
    "    def __init__(self, column_index):\n",
    "        self.index = column_index\n",
    "        self.unique_values = []\n",
    "        self.entries = []\n",
    "    \n",
    "    # add an entry to the column\n",
    "    def add(self, row_index, value):\n",
    "        self.entries.append(ColumnEntry(row_index, value))\n",
    "        if value not in self.unique_values:\n",
    "            self.unique_values.append(value)\n",
    "            \n",
    "    # get the total number of values\n",
    "    def total_values(self):\n",
    "        return len(self.unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            discrete variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        DiscreteTree instance.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best discrete variable to split the current dataset on,\n",
    "            based on the IG resulting from the split. For this best split variable, the\n",
    "            function stores several resulting attributes from the split, using the\n",
    "            store_split_values function. See the documentation of store_split_values\n",
    "            for an overview of what should be stored.\"\"\"\n",
    "        \n",
    "        # initial gain (-1 instead of 0 so that we set the info_gain at least once)\n",
    "        highest_gain = -1;\n",
    "        \n",
    "        # the total number of discrete variables \n",
    "        total_vars = self.data[0].size_discrete()\n",
    "        \n",
    "        # for every column\n",
    "        for var_index in range(total_vars):\n",
    "            \n",
    "            # create the column\n",
    "            column = DataColumn(var_index)\n",
    "            \n",
    "            # for every row\n",
    "            for row_index in range(len(self.data)):\n",
    "                \n",
    "                # get the DataRow object\n",
    "                row = self.data[row_index]\n",
    "                \n",
    "                # add the row index and value to the column\n",
    "                value = row.get_discrete(var_index)\n",
    "                column.add(row_index, value)\n",
    "            \n",
    "            # (re)set the values\n",
    "            info_gain = 0\n",
    "            indices = []\n",
    "            var_values = []\n",
    "            \n",
    "            # for every unique value in column\n",
    "            for unique_value in column.unique_values:\n",
    "                \n",
    "                # for all rows with that value create indices list\n",
    "                sub_indices = [row.index for row in column.entries if row.value == unique_value]        \n",
    "                \n",
    "                indices.append(sub_indices)\n",
    "                var_values.append(unique_value)\n",
    "                \n",
    "                # calculate information gain\n",
    "                info_gain = information_gain(self.labels, indices)\n",
    "                \n",
    "            # store the entropy if its the one with the highest gain\n",
    "            if info_gain > highest_gain:\n",
    "                self.store_split_values(var_index, var_values, indices, info_gain)\n",
    "                highest_gain = info_gain\n",
    "            \n",
    "            # remove the column from our memory\n",
    "            del column\n",
    "                \n",
    "            \n",
    "    def get_subtree(self, row):\n",
    "        \"\"\" Returns the subtree one branch down, corresponding the to value of\n",
    "            variable in the DataRow for specific variable based on which the split\n",
    "            at this node was performed.\n",
    "            Returns None if the value was not present at the split.\n",
    "                row - The DataRow object containing the values that are being\n",
    "                        classified\"\"\"\n",
    "        \n",
    "        # get the value of the correct column\n",
    "        value = row.get_discrete(self.var_index)\n",
    "        \n",
    "        # get the branch of that value\n",
    "        try:\n",
    "            return self.branches[value]\n",
    "        except:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the decision tree\n",
    "dtree = DecisionTree(train_data_rows, train_y, 1, 0.1)\n",
    "\n",
    "# print for me\n",
    "# print(dtree.var_index, dtree.var_values, dtree.indices, len(dtree.indices), dtree.info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  78 %\n"
     ]
    }
   ],
   "source": [
    "validation_x, validation_y = x_y_split(validation)\n",
    "validation_dx, validation_nx = discrete_numeric_split(validation_x)\n",
    "validation_data_rows = create_rows(validation_dx, validation_nx)\n",
    "\n",
    "precision = dtree.validate(validation_data_rows, validation_y)\n",
    "\n",
    "print(\"Validation: \", round(precision), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumericTree [3 pts]\n",
    "\n",
    "Now we move on to adding the numeric splits to the tree. All the code already written in the `DecisionTree` class, can just be inherited down to the `NumericTree` class as well, meaning you will only need to write the 2 numeric-specific functions.\n",
    "\n",
    "The most important of these function is of course the `split` function. In the numeric case, it doesn't make sense to make a separate branch for every value, because there will be many different ones, so instead some values should be grouped together for generality. The numeric split is therefore based on a split boundary, where all values smaller than the boundary go in one branch, and those greater or equal go in the other branch. `NumericTrees` thus always have exactly 2 branches for each split, but there are many different ways a single variable might be split.\n",
    "\n",
    "Write the `split` function to, for each numeric variable, try every possible split boundary and the find the split with the best *IG* overall. Try and come up with a logical way to generate all possible ways to separate a set of numeric values into 2 sets using a split boundary.\n",
    "\n",
    "When you have determined the best possible split, use the `store_split_values` function from `DecisionTree` to store most of the attributes from this split. However, as this is a numeric split, the values attribute `var_values` should not contain all variable values, but instead be set to just a list with `[False, True]`. Here *False* will be the branch with the values smaller than the boundary, and *True* the branch for the values greater or equal than the boundary. Doing this ensures that this function will still work with the `create_subtrees` you already wrote. As a last addition, you should also store as an object attribute the split boundary for this best split, as you will need to compare new values to this same boundary in order to classify them. \n",
    "\n",
    "Now write the `get_subtree` function, where you should compare the value of the variable on which the split was performed to the split boundary. Use this to get the correct subtree from the `branches` attribute, where *False* is for the smaller values and *True* for those greater or equal, as described in the section above.\n",
    "\n",
    "With these functions for the numeric split written, the `DecisionTree` class can be used to create the actual decision tree, just like for the `DiscreteTree`. Create 3 `DecisionTree` instances using the training data, with `tree_type` set to $0$, $1$ and $2$ respectively. Tree type 1 will contain only discrete splits, tree type 2 will contain only numeric splits and tree type 0 will try both and use the split with the highest information gain. Print the validation results for all 3 tree using the validation set created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericTree(DecisionTree):\n",
    "    def __init__(self, dtree):\n",
    "        \"\"\" Takes a DecisionTree as initialization parameter and copies all its\n",
    "            attributes. Then calls the split() function to determine the optimal\n",
    "            numeric variable to split this subset of the data on.\n",
    "                dtree - The DecisionTree instance whose attributes are copied to this\n",
    "                        NumericTree instance.\n",
    "            N.B. This function has already been provided and does not need to be\n",
    "            modified.\"\"\"\n",
    "        self.__dict__ = dtree.__dict__.copy()\n",
    "        self.split()\n",
    "\n",
    "    def split(self):\n",
    "        \"\"\" Determines the best boundary for any numeric variable to split the\n",
    "            current dataset on, based on the IG resulting from the split. For this\n",
    "            best split boundary, the function stores several resulting attributes\n",
    "            from the split, using the store_split_values function. See the\n",
    "            documentation of store_split_values for an overview of what should\n",
    "            be stored. In addition, one more attribute is stored in the numeric\n",
    "            case, namely the boundary value used for the split.\"\"\"\n",
    "        \n",
    "         # initial gain (-1 instead of 0 so that we set the info_gain at least once)\n",
    "        highest_gain = -1;\n",
    "        \n",
    "        # the total number of discrete variables \n",
    "        total_vars = self.data[0].size_numeric()\n",
    "        \n",
    "        # for every column\n",
    "        for var_index in range(total_vars):\n",
    "            \n",
    "            # create the column\n",
    "            column = DataColumn(var_index)\n",
    "            \n",
    "            # for every row\n",
    "            for row_index in range(len(self.data)):\n",
    "                \n",
    "                # get the DataRow object\n",
    "                row = self.data[row_index]\n",
    "                \n",
    "                # add the row index and value to the column\n",
    "                value = row.get_numeric(var_index)\n",
    "                column.add(row_index, value)\n",
    "            \n",
    "            # the var values     \n",
    "            # NOTE:  This was the bug... I had var_values = ['bigger', 'smaller'] but I add the indices that \n",
    "            #        have a smaller value first. So it ALWAYS picked the wrong branch \n",
    "            #        and still had an accuracy of over 50% \n",
    "            var_values = ['smaller', 'bigger']\n",
    "            \n",
    "            # for every unique value in column\n",
    "            for unique_value in column.unique_values:\n",
    "                \n",
    "                # (re)set the values\n",
    "                info_gain = 0\n",
    "                indices = []\n",
    "                \n",
    "                # get the indices that are smaller (or bigger of the second line) than the current unique value \n",
    "                smaller_indices = [row.index for row in column.entries if row.value < unique_value]\n",
    "                bigger_indices = [row.index for row in column.entries if row.value >= unique_value]   \n",
    "                \n",
    "                # add the sub indices to the total\n",
    "                indices.append(smaller_indices)\n",
    "                indices.append(bigger_indices)\n",
    "                \n",
    "                info_gain = information_gain(self.labels, indices)\n",
    "                 \n",
    "                # store the entropy if its the one with the highest gain\n",
    "                if info_gain > highest_gain:\n",
    "                    self.store_split_values(var_index, var_values, indices, info_gain)\n",
    "                    highest_gain = info_gain\n",
    "\n",
    "                    # also store the unique value as the boundary\n",
    "                    self.boundary = unique_value\n",
    "            \n",
    "            # remove the column from the memory\n",
    "            del column\n",
    "                    \n",
    "        \n",
    "    def get_subtree(self, row):\n",
    "        \"\"\" Returns the subtree one branch down, corresponding to the value of\n",
    "            variable in the DataRow for specific variable based on which the split\n",
    "            at this node was performed, and its corresponding boundary.\n",
    "                row - The DataRow object containing the values that are being\n",
    "                        classified\"\"\"\n",
    "        \n",
    "        # get the value of the correct column\n",
    "        value = row.get_numeric(self.var_index)\n",
    "        \n",
    "        # get the branch of that value\n",
    "        if value < self.boundary:\n",
    "            return self.branches['smaller']\n",
    "        else:\n",
    "            return self.branches['bigger']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['smaller', 'bigger'] [[1, 2, 8, 12, 18, 20, 24, 27, 29, 36, 37, 38, 39, 40, 46, 47, 51, 54, 55, 59, 60, 62, 64, 65, 68, 69, 72, 76, 77, 79, 81, 90, 91, 95, 102, 103, 107, 108, 109, 110, 112, 113, 114, 117, 118, 121, 122, 125, 126, 129, 135, 136, 137, 142, 145, 150, 152, 153, 154, 162, 167, 168, 169, 170, 172, 173, 174, 175, 181, 185, 187, 189, 190, 191, 192, 193, 196, 200, 202, 203, 204], [0, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15, 16, 17, 19, 21, 22, 23, 25, 26, 28, 30, 31, 32, 33, 34, 35, 41, 42, 43, 44, 45, 48, 49, 50, 52, 53, 56, 57, 58, 61, 63, 66, 67, 70, 71, 73, 74, 75, 78, 80, 82, 83, 84, 85, 86, 87, 88, 89, 92, 93, 94, 96, 97, 98, 99, 100, 101, 104, 105, 106, 111, 115, 116, 119, 120, 123, 124, 127, 128, 130, 131, 132, 133, 134, 138, 139, 140, 141, 143, 144, 146, 147, 148, 149, 151, 155, 156, 157, 158, 159, 160, 161, 163, 164, 165, 166, 171, 176, 177, 178, 179, 180, 182, 183, 184, 186, 188, 194, 195, 197, 198, 199, 201, 205, 206, 207, 208]] 2 0.12913097348369496 148.0\n"
     ]
    }
   ],
   "source": [
    "# create the decision tree\n",
    "ntree = DecisionTree(train_data_rows, train_y, 2, 0.0001) \n",
    "\n",
    "# print for me\n",
    "print(ntree.var_index, ntree.var_values, ntree.indices, len(ntree.indices), ntree.info_gain, ntree.boundary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  100 %\n"
     ]
    }
   ],
   "source": [
    "precision = ntree.validate(train_data_rows, train_y)\n",
    "\n",
    "print(\"Validation: \", round(precision), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# something for me to test if the tree creation worked\n",
    "def print_tree(dtree, level = 0):\n",
    "    if dtree is not None:\n",
    "        for key in dtree.branches:\n",
    "            print(level,\"\\t\",key, dtree.branches[key])\n",
    "            print_tree(dtree.branches[key], level+1)\n",
    "\n",
    "# print_tree(ntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTree(train_data_rows, train_y, 0, 0.1)\n",
    "\n",
    "precision = tree.validate(validation_data_rows, validation_y)\n",
    "\n",
    "print(\"Validation: \", round(precision), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results [2 pts]\n",
    "\n",
    "Write a function `split_data`, which takes the original cleaned data 2d-array, performs the necessary data splitting and row creation functions and returns `DataRow` and label arrays for a new training and validation set. You should already have the code to do all this, simply put it together in a function so you can easily create new validation splits.\n",
    "\n",
    "Finally, write some code to compare the results of different `DecisionTrees`. Compare the different types of trees you can make, and different values for the *IG* threshold. Repeat each of these for different validation splits and average the outcomes. Show the validation results on both the original training sets and the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):\n",
    "    \n",
    "    # create a new hard copy of the data\n",
    "    shuffled = np.copy(data)\n",
    "    \n",
    "    # shuffle the copy\n",
    "    np.random.shuffle(shuffled)\n",
    "    \n",
    "    # create a train and validation set\n",
    "    train, validation = validation_split(shuffled, 0.7)\n",
    "    \n",
    "    # create train data rows and labels\n",
    "    train_x, train_y = x_y_split(train)\n",
    "    train_dx, train_nx = discrete_numeric_split(train_x)\n",
    "    train_data_rows = create_rows(train_dx, train_nx)\n",
    "\n",
    "    # create validation data rows and labels\n",
    "    validation_x, validation_y = x_y_split(validation)\n",
    "    validation_dx, validation_nx = discrete_numeric_split(validation_x)\n",
    "    validation_data_rows = create_rows(validation_dx, validation_nx)\n",
    "    \n",
    "    return train_data_rows, train_y, validation_data_rows, validation_y\n",
    "\n",
    "\n",
    "def compare_trees(data, min_thres, max_thres, thres_step_size, dingen):\n",
    "    \n",
    "    precisions = np.zeros((2, 3))\n",
    "    thresholds = np.arange(min_thres, max_thres, thres_step_size)\n",
    "    total = len(thresholds)*dingen\n",
    "    \n",
    "    for tree_type in range(3):\n",
    "        \n",
    "        train_precision = 0\n",
    "        validation_precision = 0\n",
    "        \n",
    "        for thres in thresholds:\n",
    "            \n",
    "            print(\"\\nThreshold: \", thres)\n",
    "            temp_train_precision = 0\n",
    "            temp_validation_precision = 0\n",
    "            \n",
    "            for ding in range(dingen):\n",
    "        \n",
    "                train_data_rows, train_y, validation_data_rows, validation_y = split_data(data)\n",
    "        \n",
    "                tree = DecisionTree(train_data_rows, train_y, tree_type, thres)\n",
    "        \n",
    "                temp_train_precision += tree.validate(train_data_rows, train_y)\n",
    "                temp_validation_precision += tree.validate(validation_data_rows, validation_y)\n",
    "                \n",
    "            train_precision += temp_train_precision\n",
    "            validation_precision += temp_validation_precision\n",
    "                \n",
    "            if tree_type == 0:\n",
    "                print(\"Validation train combined: \", round(temp_train_precision / dingen), \"%\")\n",
    "                print(\"Validation validate combined: \", round(temp_validation_precision / dingen), \"%\")\n",
    "            elif tree_type == 1:\n",
    "                print(\"Validation train discrete: \", round(temp_train_precision / dingen), \"%\")\n",
    "                print(\"Validation validate discrete: \", round(temp_validation_precision / dingen), \"%\")\n",
    "            else :\n",
    "                print(\"Validation train numeric: \", round(temp_train_precision / dingen), \"%\")\n",
    "                print(\"Validation validate numeric: \", round(temp_validation_precision / dingen), \"%\")\n",
    "                \n",
    "        precisions[0,tree_type] = train_precision / total\n",
    "        precisions[1,tree_type] = validation_precision / total\n",
    "        \n",
    "    return precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold:  0.01\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  75 %\n",
      "\n",
      "Threshold:  0.02\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  72 %\n",
      "\n",
      "Threshold:  0.03\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  73 %\n",
      "\n",
      "Threshold:  0.04\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  74 %\n",
      "\n",
      "Threshold:  0.05\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  74 %\n",
      "\n",
      "Threshold:  0.060000000000000005\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  74 %\n",
      "\n",
      "Threshold:  0.06999999999999999\n",
      "Validation train combined:  99 %\n",
      "Validation validate combined:  74 %\n",
      "\n",
      "Threshold:  0.08\n",
      "Validation train combined:  100 %\n",
      "Validation validate combined:  74 %\n",
      "\n",
      "Threshold:  0.09\n",
      "Validation train combined:  99 %\n",
      "Validation validate combined:  72 %\n",
      "\n",
      "Threshold:  0.01\n",
      "Validation train discrete:  95 %\n",
      "Validation validate discrete:  77 %\n",
      "\n",
      "Threshold:  0.02\n",
      "Validation train discrete:  95 %\n",
      "Validation validate discrete:  77 %\n",
      "\n",
      "Threshold:  0.03\n",
      "Validation train discrete:  95 %\n",
      "Validation validate discrete:  75 %\n",
      "\n",
      "Threshold:  0.04\n",
      "Validation train discrete:  95 %\n",
      "Validation validate discrete:  76 %\n",
      "\n",
      "Threshold:  0.05\n",
      "Validation train discrete:  94 %\n",
      "Validation validate discrete:  77 %\n",
      "\n",
      "Threshold:  0.060000000000000005\n",
      "Validation train discrete:  94 %\n",
      "Validation validate discrete:  76 %\n",
      "\n",
      "Threshold:  0.06999999999999999\n",
      "Validation train discrete:  94 %\n",
      "Validation validate discrete:  78 %\n",
      "\n",
      "Threshold:  0.08\n",
      "Validation train discrete:  93 %\n",
      "Validation validate discrete:  76 %\n",
      "\n",
      "Threshold:  0.09\n",
      "Validation train discrete:  92 %\n",
      "Validation validate discrete:  77 %\n",
      "\n",
      "Threshold:  0.01\n",
      "Validation train numeric:  100 %\n",
      "Validation validate numeric:  65 %\n",
      "\n",
      "Threshold:  0.02\n",
      "Validation train numeric:  100 %\n",
      "Validation validate numeric:  65 %\n",
      "\n",
      "Threshold:  0.03\n",
      "Validation train numeric:  100 %\n",
      "Validation validate numeric:  64 %\n",
      "\n",
      "Threshold:  0.04\n",
      "Validation train numeric:  98 %\n",
      "Validation validate numeric:  65 %\n",
      "\n",
      "Threshold:  0.05\n",
      "Validation train numeric:  93 %\n",
      "Validation validate numeric:  67 %\n",
      "\n",
      "Threshold:  0.060000000000000005\n",
      "Validation train numeric:  88 %\n",
      "Validation validate numeric:  68 %\n",
      "\n",
      "Threshold:  0.06999999999999999\n",
      "Validation train numeric:  85 %\n",
      "Validation validate numeric:  68 %\n",
      "\n",
      "Threshold:  0.08\n",
      "Validation train numeric:  83 %\n",
      "Validation validate numeric:  68 %\n",
      "\n",
      "Threshold:  0.09\n",
      "Validation train numeric:  80 %\n",
      "Validation validate numeric:  69 %\n"
     ]
    }
   ],
   "source": [
    "validations = compare_trees(complete_data, 0.01, 0.1, 0.01, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation train combined:  100.0 %\n",
      "Validation train discrete:  94.0 %\n",
      "Validation train numeric:  92.0 %\n",
      "\n",
      "Validation validate combined:  74.0 %\n",
      "Validation validate discrete:  76.0 %\n",
      "Validation validate numeric:  66.0 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation train combined: \", round(validations[0, 0]), \"%\")\n",
    "print(\"Validation train discrete: \", round(validations[0, 1]), \"%\")\n",
    "print(\"Validation train numeric: \", round(validations[0, 2]), \"%\")\n",
    "print()\n",
    "print(\"Validation validate combined: \", round(validations[1, 0]), \"%\")\n",
    "print(\"Validation validate discrete: \", round(validations[1, 1]), \"%\")\n",
    "print(\"Validation validate numeric: \", round(validations[1, 2]), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis questions [2 pts]\n",
    "\n",
    "If your algorithm is correct and you averaged over enough different validation splits, you might see some strange results in the comparison you just produced. For the last part of the assignment, answer these questions about the results. Write you answers below each question in this cell.\n",
    "\n",
    "#### Why does the validation score on the training set for numeric and mixed splits eventually reach 100% correct, but never for just discrete splits?\n",
    "\n",
    "The discrete splits can only happen for once per variable/column, since we make branches for the seperate values. We can create multiple bigger/smaller splits on the numeric variables and increase the certainty of seen data even more i.e. group them more/closer together. It only happens when the threshold is really really small.\n",
    "\n",
    "\n",
    "#### Can you explain how it is possible that the validation score using just the discrete variables is higher than the validation score using the discrete and numeric variables combined? What property of the algorithm makes this outcome possible?\n",
    "\n",
    "Because the decision tree branches have a better split for new data. You get more specific branches that have more meaning than the numeric split.\n",
    "\n",
    "#### What is your hypothesis for why this happens for this particular data? What could you do to improve the validation results?\n",
    "The numeric variables overfit. The numeric variables need a higher threshold than the discrete variables, so you can use differrent thresholds for discrete and numeric values to prevent over- and underfitting. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
